{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://download.pytorch.org/whl/cu117\n",
      "Requirement already satisfied: torch in c:\\python\\lib\\site-packages (2.5.0)\n",
      "Requirement already satisfied: torchvision in c:\\python\\lib\\site-packages (0.19.1)\n",
      "Requirement already satisfied: torchaudio in c:\\python\\lib\\site-packages (2.5.0)\n",
      "Requirement already satisfied: filelock in c:\\python\\lib\\site-packages (from torch) (3.16.1)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in c:\\python\\lib\\site-packages (from torch) (4.12.2)\n",
      "Requirement already satisfied: networkx in c:\\python\\lib\\site-packages (from torch) (3.4)\n",
      "Requirement already satisfied: jinja2 in c:\\python\\lib\\site-packages (from torch) (3.1.4)\n",
      "Requirement already satisfied: fsspec in c:\\python\\lib\\site-packages (from torch) (2024.9.0)\n",
      "Requirement already satisfied: setuptools in c:\\python\\lib\\site-packages (from torch) (75.1.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in c:\\python\\lib\\site-packages (from torch) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\python\\lib\\site-packages (from sympy==1.13.1->torch) (1.3.0)\n",
      "Requirement already satisfied: numpy in c:\\python\\lib\\site-packages (from torchvision) (1.26.4)\n",
      "Collecting torch\n",
      "  Using cached torch-2.4.1-cp312-cp312-win_amd64.whl.metadata (27 kB)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in c:\\python\\lib\\site-packages (from torchvision) (10.4.0)\n",
      "INFO: pip is looking at multiple versions of torchaudio to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting torchaudio\n",
      "  Downloading torchaudio-2.5.1-cp312-cp312-win_amd64.whl.metadata (6.5 kB)\n",
      "  Downloading torchaudio-2.4.1-cp312-cp312-win_amd64.whl.metadata (6.5 kB)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\python\\lib\\site-packages (from jinja2->torch) (2.1.5)\n",
      "Using cached torch-2.4.1-cp312-cp312-win_amd64.whl (199.4 MB)\n",
      "Downloading torchaudio-2.4.1-cp312-cp312-win_amd64.whl (2.4 MB)\n",
      "   ---------------------------------------- 0.0/2.4 MB ? eta -:--:--\n",
      "   ---------------------------------------- 2.4/2.4 MB 17.2 MB/s eta 0:00:00\n",
      "Installing collected packages: torch, torchaudio\n",
      "  Attempting uninstall: torch\n",
      "    Found existing installation: torch 2.5.0\n",
      "    Uninstalling torch-2.5.0:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: Could not install packages due to an OSError: [WinError 5] Access is denied: 'c:\\\\python\\\\lib\\\\site-packages\\\\torch\\\\lib\\\\asmjit.dll'\n",
      "Consider using the `--user` option or check the permissions.\n",
      "\n",
      "\n",
      "[notice] A new release of pip is available: 24.2 -> 24.3.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "!pip install torch torchvision torchaudio --extra-index-url https://download.pytorch.org/whl/cu117"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import torch\n",
    "\n",
    "# Load the tokenizer and model\n",
    "model_name = \"D:\\\\Math Mentor AI\\\\llama-3.2-3b-mathdaily-chatbot\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name).to(\"cuda\")\n",
    "\n",
    "# Set up the prompt and question\n",
    "instruction_prompt = \"You are a helpful Mathematics Professor for 7th grade students. Your job is to give hints through other examples and help students get to the right answer without revealing the answer directly. Help the student get to the solution for the given question: \"\n",
    "question = \"\"\"\n",
    "A rectangle has a length of 12 cm and a width of 8 cm.  \n",
    "If you increase both the length and width by 3 cm, what will be the new area of the rectangle?  \n",
    "Calculate the original area and the new area.  \n",
    "How much does the area increase?\n",
    "\"\"\"\n",
    "\n",
    "# Concatenate the instruction and question as a single prompt\n",
    "instruction = f\"{instruction_prompt}{question}\"\n",
    "\n",
    "# Simulate the chat-based prompt (no `apply_chat_template` available directly here)\n",
    "user_input = \"I think the answer is 21\"\n",
    "chat_prompt = f\"{instruction}\\n\\nStudent: {user_input}\\nProfessor:\"\n",
    "\n",
    "# Tokenize the prompt and set up input for model generation\n",
    "inputs = tokenizer(chat_prompt, return_tensors='pt', padding=True, truncation=True).to(\"cuda\")\n",
    "\n",
    "# Generate the response\n",
    "outputs = model.generate(\n",
    "    **inputs,\n",
    "    max_new_tokens=150,\n",
    "    num_return_sequences=1,\n",
    "    pad_token_id=tokenizer.eos_token_id,  # Set pad token id if eos token is used for padding\n",
    "    temperature=0.7,  # Optional, to adjust response creativity\n",
    ")\n",
    "\n",
    "# Decode and format the generated response\n",
    "generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "response = generated_text.split(\"Professor:\")[1].strip()  # Extract response following \"Professor:\"\n",
    "\n",
    "print(response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
