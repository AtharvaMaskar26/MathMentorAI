{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "import streamlit as st\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from jax import random, jit\n",
    "import jax.numpy as jnp\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open(\"../data/question_answer_pairs.json\", \"r\") as jsonFile:\n",
    "    dataset = json.load(jsonFile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "35"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constants\n",
    "n_arms = len(dataset) # Number of question types (arms)\n",
    "context_dim = 3  # Number of context features\n",
    "exploration_threshold = 5  # Number of exploratory questions before switching to exploitation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize parameters\n",
    "weights = jnp.zeros((n_arms, context_dim))\n",
    "learning_rate = 0.1\n",
    "gamma = 0.95"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Q-values and counts\n",
    "Q_values = jnp.zeros(n_arms)\n",
    "counts = jnp.zeros(n_arms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store last attempted timestamps for (type, level) combinations\n",
    "last_attempted_timestamps = {(type_, level): None for type_ in range(6) for level in range(1, 6)}\n",
    "\n",
    "# Initialize counters\n",
    "total_questions = 0\n",
    "correct_answers = 0\n",
    "hints_used = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "@jit\n",
    "def contextual_bandit_selection(weights, context, exploration_count):\n",
    "    if exploration_count < exploration_threshold:\n",
    "        return np.random.randint(n_arms), jnp.zeros(n_arms)  # Random selection for exploration\n",
    "    expected_rewards = jnp.dot(weights, context)\n",
    "    return jnp.argmax(expected_rewards), expected_rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_time_since_last_attempt():\n",
    "    # Calculate time since last attempt\n",
    "    if total_questions == 0:\n",
    "        return 0.0  # If never attempted, use 0\n",
    "    else:\n",
    "        # Assuming the last attempt timestamp is stored\n",
    "        last_time = last_attempted_timestamps.get('last_attempt', datetime.now())\n",
    "        time_since_last_attempt = (datetime.now() - last_time).total_seconds()  # in seconds\n",
    "        return min(time_since_last_attempt / 3600.0, 24.0)  # Normalize to hours (capped at 24 hours)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "@jit\n",
    "def update_Q_values(arm_index, reward, Q_values, expected_rewards, gamma, learning_rate):\n",
    "    best_next_reward = jnp.max(expected_rewards)\n",
    "    Q_values = Q_values.at[arm_index].set(\n",
    "        Q_values[arm_index] + learning_rate * (reward + gamma * best_next_reward - Q_values[arm_index])\n",
    "    )\n",
    "    return Q_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "recent_performace: ration of correct answers\n",
    "timestamp_since_last_question_practiced\n",
    "number_of_hints_per_questions\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>problem</th>\n",
       "      <th>level</th>\n",
       "      <th>type</th>\n",
       "      <th>solution</th>\n",
       "      <th>stage</th>\n",
       "      <th>source</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Kevin Kangaroo begins hopping on a number line...</td>\n",
       "      <td>Level 5</td>\n",
       "      <td>Algebra</td>\n",
       "      <td>Kevin hops $1/3$ of the remaining distance wit...</td>\n",
       "      <td>train</td>\n",
       "      <td>MATH</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>The ratio of the areas of two squares is $\\fra...</td>\n",
       "      <td>Level 4</td>\n",
       "      <td>Algebra</td>\n",
       "      <td>We start off by simplifying the ratio $\\frac{1...</td>\n",
       "      <td>train</td>\n",
       "      <td>MATH</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             problem    level     type  \\\n",
       "0  Kevin Kangaroo begins hopping on a number line...  Level 5  Algebra   \n",
       "1  The ratio of the areas of two squares is $\\fra...  Level 4  Algebra   \n",
       "\n",
       "                                            solution  stage source  \n",
       "0  Kevin hops $1/3$ of the remaining distance wit...  train   MATH  \n",
       "1  We start off by simplifying the ratio $\\frac{1...  train   MATH  "
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_contextual_bandit(recent_performance, time_of_day, session_count):\n",
    "    global weights, Q_values, question_count\n",
    "    \n",
    "    # Prepare context features\n",
    "    context = jnp.array([recent_performance, time_of_day, session_count, question_count])\n",
    "    \n",
    "    # Select question using contextual bandit\n",
    "    arm_index, expected_rewards = contextual_bandit_selection(weights, context, question_count)\n",
    "    \n",
    "    selected_question = questions.iloc[arm_index]\n",
    "    \n",
    "    # Simulate user response (for demonstration purposes)\n",
    "    correct = np.random.rand() < 0.5  # Randomly determining correctness for simulation\n",
    "    reward = 1.0 if correct else 0.0  # Basic reward structure for demonstration\n",
    "\n",
    "    # Update Q-values and weights\n",
    "    Q_values = update_Q_values(arm_index, reward, Q_values, expected_rewards, gamma, learning_rate)\n",
    "    weights = weights.at[arm_index].set(weights[arm_index] + learning_rate * reward * context)\n",
    "\n",
    "    # Update the timestamp for the problem type and level\n",
    "    question_count += 1\n",
    "\n",
    "    return selected_question['question'], selected_question['solution'], correct\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "0/1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax.numpy as jnp\n",
    "import jax.random as jrandom\n",
    "import streamlit as st\n",
    "\n",
    "# Constants\n",
    "NUM_QUESTIONS = 35  # Total number of questions\n",
    "NUM_ARS = 3  # Number of arms (question sets)\n",
    "INITIAL_Q_VALUES = 0.5  # Initial Q-values\n",
    "EXPLORATION_RATE = 0.2  # Probability of exploration\n",
    "\n",
    "# Initialize Q-values and counts using JAX\n",
    "Q_values = jnp.full((NUM_QUESTIONS, NUM_ARS), INITIAL_Q_VALUES)\n",
    "counts = jnp.zeros((NUM_QUESTIONS, NUM_ARS))\n",
    "\n",
    "# Function to select a question using epsilon-greedy strategy\n",
    "def select_question(rng_key, context):\n",
    "    if jrandom.uniform(rng_key) < EXPLORATION_RATE:\n",
    "        # Explore: select a random arm\n",
    "        arm_index = jrandom.randint(rng_key, (1,), 0, NUM_ARS)[0]\n",
    "    else:\n",
    "        # Exploit: select the best arm based on Q-values\n",
    "        arm_index = jnp.argmax(Q_values[context])\n",
    "    return arm_index\n",
    "\n",
    "# Custom reward function based on your criteria\n",
    "def custom_reward(is_correct, hints_used, timestamp_since_last, correct_ratio):\n",
    "    if is_correct:\n",
    "        base_reward = 1.0  # Base reward for a correct answer\n",
    "        hint_penalty = 0.2 * hints_used  # Penalty per hint used (0-5)\n",
    "        reward = base_reward - hint_penalty\n",
    "        \n",
    "        # Deduct penalty for attempting too soon\n",
    "        if timestamp_since_last < 10:  # less than 10 minutes\n",
    "            reward -= 0.5\n",
    "        \n",
    "        # Scale reward based on correct answer ratio\n",
    "        reward += 0.5 * correct_ratio  # Scale with ratio (0-1)\n",
    "        \n",
    "    else:\n",
    "        reward = 0.0  # No reward for incorrect answers\n",
    "    \n",
    "    return max(reward, 0.0)  # Ensure reward is not negative\n",
    "\n",
    "# Function to update Q-values using the reward received\n",
    "def update_q_values(Q_values, counts, context, arm_index, reward):\n",
    "    counts = counts.at[context, arm_index].add(1)  # Increment counts\n",
    "    # Update Q-values using the formula\n",
    "    Q_values = Q_values.at[context, arm_index].add((reward - Q_values[context, arm_index]) / counts[context, arm_index])\n",
    "    return Q_values, counts\n",
    "\n",
    "# Main function to run the Streamlit app\n",
    "def main():\n",
    "    st.title(\"Math Mentor AI - Contextual Bandit\")\n",
    "    \n",
    "    # User inputs\n",
    "    recent_performance = st.slider(\"Recent Performance Ratio (0-1)\", 0.0, 1.0, 0.5)\n",
    "    hints_used = st.slider(\"Number of Hints Used (0-5)\", 0, 5, 2)\n",
    "    timestamp_since_last = st.slider(\"Time Since Last Attempt (minutes)\", 0, 30, 5)\n",
    "    correct_ratio = st.slider(\"Correct Answer Ratio (0-1)\", 0.0, 1.0, 0.5)\n",
    "\n",
    "    # Prepare context features\n",
    "    context = int(recent_performance * (NUM_QUESTIONS - 1))  # Map performance to context\n",
    "    \n",
    "    # Generate a random key for JAX\n",
    "    rng_key = jrandom.PRNGKey(0)  # You might want to change the seed for each run\n",
    "    arm_index = select_question(rng_key, context)  # Select question\n",
    "\n",
    "    st.write(f\"Suggested Question Index: {arm_index}\")\n",
    "\n",
    "    # Simulate user's answer\n",
    "    is_correct = st.radio(\"Did you answer correctly?\", (\"Yes\", \"No\"))\n",
    "    is_correct = 1 if is_correct == \"Yes\" else 0\n",
    "\n",
    "    # Calculate the reward based on user's response\n",
    "    reward = custom_reward(is_correct, hints_used, timestamp_since_last, correct_ratio)\n",
    "    \n",
    "    # Update Q-values with the received reward\n",
    "    global Q_values, counts\n",
    "    Q_values, counts = update_q_values(Q_values, counts, context, arm_index, reward)\n",
    "\n",
    "    st.write(f\"Reward Received: {reward}\")\n",
    "\n",
    "# Run the Streamlit app\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "# Initialize Q-values and counts\n",
    "# Q_values = jnp.zeros((NUM_CONTEXTS, NUM_ARMS))  # All Q-values initialized to 0\n",
    "# counts = jnp.zeros((NUM_CONTEXTS, NUM_ARMS))  # All counts initialized to 0\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Main training loop\n",
    "def train_bandit():\n",
    "    rng_key = jrandom.PRNGKey(0)  # Random key for reproducibility\n",
    "    global Q_values, counts\n",
    "    \n",
    "    for episode in range(NUM_EPISODES):\n",
    "        context_index = jrandom.randint(rng_key, (1,), 0, NUM_CONTEXTS)[0]  # Randomly select a context\n",
    "        arm_index = select_question(rng_key, Q_values, context_index)  # Select a question/arm\n",
    "        \n",
    "        # Simulate user response (for demonstration purposes)\n",
    "        # In practice, replace this with actual user interactions and feedback\n",
    "        is_correct = jrandom.uniform(rng_key) < 0.5  # Randomly simulating correctness\n",
    "        hints_used = jrandom.randint(rng_key, (1,), 0, 3)[0]  # Randomly simulate hints used\n",
    "        timestamp_since_last = jrandom.randint(rng_key, (1,), 1, 20)[0]  # Randomly simulate time since last question\n",
    "        correct_ratio = 0.5  # Simulated correct ratio, should be calculated based on history\n",
    "        \n",
    "        # Calculate the reward based on the simulated user response\n",
    "        reward = custom_reward(is_correct, hints_used, timestamp_since_last, correct_ratio)\n",
    "        \n",
    "        # Update Q-values and counts\n",
    "        Q_values, counts = update_q_values(Q_values, counts, context_index, arm_index, reward)\n",
    "\n",
    "    return Q_values, counts\n",
    "\n",
    "# Execute the training\n",
    "final_Q_values, final_counts = train_bandit()\n",
    "print(\"Final Q-values:\\n\", final_Q_values)\n",
    "print(\"Final counts:\\n\", final_counts)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax\n",
    "import jax.numpy as jnp\n",
    "from jax import random as jrandom\n",
    "\n",
    "# Constants\n",
    "NUM_CONTEXTS = 3  # Example number of contexts\n",
    "NUM_ARMS = 35      # Example number of arms (questions)\n",
    "EXPLORATION_RATE = 0.1  # Epsilon for the epsilon-greedy strategy\n",
    "\n",
    "# Initialize Q-values and counts\n",
    "Q_values = jnp.zeros((NUM_CONTEXTS, NUM_ARMS))  # All Q-values initialized to 0\n",
    "counts = jnp.zeros((NUM_CONTEXTS, NUM_ARMS))  # All counts initialized to 0\n",
    "\n",
    "# Function to select a question using epsilon-greedy strategy\n",
    "@jax.jit\n",
    "def select_question(rng_key, Q_values, context_index):\n",
    "    if jrandom.uniform(rng_key) < EXPLORATION_RATE:\n",
    "        arm_index = jrandom.randint(rng_key, (1,), 0, NUM_ARMS)[0]\n",
    "    else:\n",
    "        arm_index = jnp.argmax(Q_values[context_index])\n",
    "    return arm_index\n",
    "\n",
    "# Custom reward function based on user response and context\n",
    "@jax.jit\n",
    "def custom_reward(is_correct, hints_used, timestamp_since_last, correct_ratio):\n",
    "    if is_correct:\n",
    "        base_reward = 1.0\n",
    "        hint_penalty = 0.2 * hints_used\n",
    "        reward = base_reward - hint_penalty\n",
    "        \n",
    "        if timestamp_since_last < 10:\n",
    "            reward -= 0.5\n",
    "        \n",
    "        reward += 0.5 * correct_ratio\n",
    "    else:\n",
    "        reward = 0.0\n",
    "    \n",
    "    return max(reward, 0.0)\n",
    "\n",
    "# Function to update Q-values based on received reward\n",
    "@jax.jit\n",
    "def update_q_values(Q_values, counts, context_index, arm_index, reward):\n",
    "    counts = counts.at[context_index, arm_index].add(1)\n",
    "    Q_values = Q_values.at[context_index, arm_index].add((reward - Q_values[context_index, arm_index]) / counts[context_index, arm_index])\n",
    "    return Q_values, counts\n",
    "\n",
    "# Main training loop\n",
    "def train_bandit():\n",
    "    rng_key = jrandom.PRNGKey(0)  # Random key for reproducibility\n",
    "    global Q_values, counts\n",
    "    \n",
    "    for episode in range(NUM_EPISODES):\n",
    "        context_index = jrandom.randint(rng_key, (1,), 0, NUM_CONTEXTS)[0]  # Randomly select a context\n",
    "        arm_index = select_question(rng_key, Q_values, context_index)  # Select a question/arm\n",
    "        \n",
    "        # Simulate user response (for demonstration purposes)\n",
    "        # In practice, replace this with actual user interactions and feedback\n",
    "        is_correct = jrandom.uniform(rng_key) < 0.5  # Randomly simulating correctness\n",
    "        hints_used = jrandom.randint(rng_key, (1,), 0, 3)[0]  # Randomly simulate hints used\n",
    "        timestamp_since_last = jrandom.randint(rng_key, (1,), 1, 20)[0]  # Randomly simulate time since last question\n",
    "        correct_ratio = 0.5  # Simulated correct ratio, should be calculated based on history\n",
    "        \n",
    "        # Calculate the reward based on the simulated user response\n",
    "        reward = custom_reward(is_correct, hints_used, timestamp_since_last, correct_ratio)\n",
    "        \n",
    "        # Update Q-values and counts\n",
    "        Q_values, counts = update_q_values(Q_values, counts, context_index, arm_index, reward)\n",
    "\n",
    "    return Q_values, counts\n",
    "\n",
    "# Execute the training\n",
    "final_Q_values, final_counts = train_bandit()\n",
    "print(\"Final Q-values:\\n\", final_Q_values)\n",
    "print(\"Final counts:\\n\", final_counts)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CSV file 'user_initial_data.csv' created successfully.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Create a sample DataFrame\n",
    "data = {\n",
    "    'user_id': ['user1', 'user2', 'user3'],\n",
    "    'recent_performance': [0.5, 0.7, 0.8],\n",
    "    'hints_used': [2, 1, 3],\n",
    "    'timestamp_since_last': [5, 3, 2],\n",
    "    'correct_ratio': [0.5, 0.8, 0.9]\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Save the DataFrame to a CSV file\n",
    "USER_INITIAL_DATA_FILE = 'user_initial_data.csv'  # Specify your file path\n",
    "df.to_csv(USER_INITIAL_DATA_FILE, index=False)\n",
    "\n",
    "print(f\"CSV file '{USER_INITIAL_DATA_FILE}' created successfully.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
